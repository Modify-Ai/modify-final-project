import logging
import base64
import asyncio
import re
from typing import Optional, List, Dict, Any
from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, Form
from sqlalchemy.ext.asyncio import AsyncSession
import httpx
from pydantic import BaseModel, ValidationError 

from src.api import deps
from src.crud.crud_product import crud_product
from src.schemas.product import ProductResponse
from src.config.settings import settings
from src.constants import ProductCategory

logger = logging.getLogger(__name__)
router = APIRouter()

# ------------------------------------------------------------------
# DTO Definitions
# ------------------------------------------------------------------

class ImageAnalysisRequest(BaseModel):
    image_b64: str
    query: str

class ClipSearchRequest(BaseModel):
    image_b64: str
    limit: int = 12
    query: Optional[str] = None  # ì›ë³¸ ê²€ìƒ‰ì–´ (ì„±ë³„ ì¶”ì¶œìš©)
    target: str = "full"  # "full", "upper", "lower"

# ------------------------------------------------------------------
# Helper Functions
# ------------------------------------------------------------------

def detect_gender_intent(query: str) -> Optional[str]:
    """ê²€ìƒ‰ì–´ì—ì„œ ì„±ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    if not query:
        return None
    q = query.lower()
    if any(x in q for x in ["ë‚¨ì", "ë‚¨ì„±", "ë§¨", "men", "male", "boy"]):
        return "Male"
    elif any(x in q for x in ["ì—¬ì", "ì—¬ì„±", "ìš°ë¨¼", "women", "female", "girl"]):
        return "Female"
    return None

def extract_core_keyword(query: str) -> str:
    """ê²€ìƒ‰ì–´ì—ì„œ í•µì‹¬ ìƒí’ˆ í‚¤ì›Œë“œ ì¶”ì¶œ (ì„±ë³„/ìˆ˜ì‹ì–´ ì œê±°)"""
    if not query:
        return ""
        
    # ì œê±°í•  ë‹¨ì–´ë“¤
    remove_words = [
        "ë‚¨ì", "ì—¬ì", "ë‚¨ì„±", "ì—¬ì„±", "ë‚¨ì„±ìš©", "ì—¬ì„±ìš©",
        "ì¶”ì²œ", "í•´ì¤˜", "ë³´ì—¬ì¤˜", "ì°¾ì•„ì¤˜", "ì•Œë ¤ì¤˜",
        "ìŠ¤íƒ€ì¼", "íŒ¨ì…˜", "ì˜ë¥˜", "ìš©"
    ]
    # "ì˜·"ì€ ì œê±°í•˜ì§€ ì•ŠìŒ - ê²€ìƒ‰ì–´ë¡œ ìœ ì˜ë¯¸í•¨
    
    result = query
    for word in remove_words:
        result = result.replace(word, "")
    
    # ì¡°ì‚¬ ì œê±° (ì€/ëŠ”/ì´/ê°€ ë“±)
    result = re.sub(r'(ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì˜|ì—|ë¡œ)$', '', result.strip())
    
    return result.strip() if result.strip() else query

def is_celebrity_search(query: str) -> bool:
    """ì—°ì˜ˆì¸/ì¸ë¬¼ ê²€ìƒ‰ì¸ì§€ íŒë‹¨"""
    if not query:
        return False
        
    # íŒ¨ì…˜ ê´€ë ¨ í‚¤ì›Œë“œì™€ í•¨ê»˜ ì‚¬ìš©ëœ ê²½ìš°
    fashion_keywords = ["íŒ¨ì…˜", "ìŠ¤íƒ€ì¼", "ì½”ë””", "ë£©", "ê³µí•­", "ì°©ì¥", "ì˜ìƒ", "ì˜·"]
    
    # í•œê¸€ ì´ë¦„ íŒ¨í„´ (2-4ê¸€ì)
    korean_name = re.search(r'[ê°€-í£]{2,4}', query)
    
    if korean_name and any(k in query for k in fashion_keywords):
        return True
    
    return False

async def fetch_image_as_base64(url: str) -> Optional[str]:
    """ì™¸ë¶€ ì´ë¯¸ì§€ í”„ë¡ì‹œ ë‹¤ìš´ë¡œë“œ"""
    if not url:
        return None
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Referer": "https://www.google.com/"
        }
        async with httpx.AsyncClient(timeout=5.0, verify=False) as client:
            response = await client.get(url, headers=headers)
            if response.status_code == 200:
                b64_data = base64.b64encode(response.content).decode('utf-8')
                content_type = response.headers.get("content-type", "image/jpeg")
                return f"data:{content_type};base64,{b64_data}"
    except Exception as e:
        logger.warning(f"âš ï¸ Failed to proxy image ({url}): {e}")
    return None

def filter_by_negative_prompt(products: List[Any], negative_prompt: Optional[str]) -> List[Any]:
    """ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸ë¡œ ìƒí’ˆ í•„í„°ë§"""
    if not negative_prompt or not negative_prompt.strip():
        return products

    # ë„¤ê±°í‹°ë¸Œ í‚¤ì›Œë“œ íŒŒì‹± (ì‰¼í‘œë¡œ êµ¬ë¶„, ê³µë°± ì œê±°, ì†Œë¬¸ì ë³€í™˜)
    negative_keywords = [
        kw.strip().lower()
        for kw in negative_prompt.split(',')
        if kw.strip()
    ]

    if not negative_keywords:
        return products

    logger.info(f"ğŸš« Negative keywords: {negative_keywords}")

    # í•„í„°ë§ ë¡œì§
    filtered_products = []
    for product in products:
        # ìƒí’ˆ í…ìŠ¤íŠ¸ ê²°í•© (ì´ë¦„, ì„¤ëª…, ì¹´í…Œê³ ë¦¬)
        searchable_text = " ".join([
            (product.name or "").lower(),
            (product.description or "").lower(),
            (product.category or "").lower()
        ])

        # ë„¤ê±°í‹°ë¸Œ í‚¤ì›Œë“œê°€ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ë©´ ì œì™¸
        if any(keyword in searchable_text for keyword in negative_keywords):
            logger.debug(f"ğŸš« Filtered out: {product.name} (matched negative keyword)")
            continue

        filtered_products.append(product)

    logger.info(f"âœ… Negative filtering: {len(products)} â†’ {len(filtered_products)} products")
    return filtered_products


def map_product_to_response(product) -> Optional[ProductResponse]:
    """Product ê°ì²´ë¥¼ ProductResponseë¡œ ë³€í™˜ (similarity í¬í•¨)"""
    try:
        p_dict = {
            "id": product.id,
            "name": product.name or "Unnamed Product",
            "description": product.description or "",
            "price": float(product.price) if product.price else 0,
            "stock_quantity": int(product.stock_quantity) if product.stock_quantity else 0,
            "category": product.category or "Etc",
            "image_url": product.image_url or "",
            "gender": product.gender or "Unisex",
            "is_active": product.is_active if product.is_active is not None else True,
            "created_at": product.created_at,
            "updated_at": product.updated_at,
            "in_stock": (product.stock_quantity or 0) > 0,
            "similarity": getattr(product, 'similarity', None)
        }
        return ProductResponse.model_validate(p_dict)
    except ValidationError as e:
        logger.warning(f"âš ï¸ Product validation error: {e}")
        return None


# ------------------------------------------------------------------
# Endpoints
# ------------------------------------------------------------------

@router.post("/search-by-clip")
async def search_by_clip_image(
    request: ClipSearchRequest,
    db: AsyncSession = Depends(deps.get_db),
):
    """
    ì´ë¯¸ì§€ ê¸°ë°˜ ìƒí’ˆ ê²€ìƒ‰ (CLIP Vector)
    """
    logger.info(f"ğŸ–¼ï¸ CLIP Image Search Request (limit: {request.limit}, query: {request.query}, target: {request.target})")
    
    # 1. ì›ë³¸ ì¿¼ë¦¬ì—ì„œ ì„±ë³„ ì¶”ì¶œ
    target_gender = None
    if request.query:
        target_gender = detect_gender_intent(request.query)
        logger.info(f"ğŸ“Œ Detected gender from query: {target_gender}")

    target_categories = None
    if request.target == "upper":
        target_categories = [
            ProductCategory.TOPS.value, 
            ProductCategory.OUTERWEAR.value, 
            ProductCategory.DRESSES.value
        ]
    elif request.target == "lower":
        target_categories = [ProductCategory.BOTTOMS.value] 
    
    # âœ… [FIX] URL ë³´ì • ë¡œì§ ì¶”ê°€
    raw_url = settings.AI_SERVICE_API_URL.rstrip("/")
    if "/api/v1" not in raw_url:
        AI_SERVICE_API_URL = f"{raw_url}/api/v1"
    else:
        AI_SERVICE_API_URL = raw_url
    
    try:
        # 2. AI ì„œë¹„ìŠ¤ì—ì„œ CLIP ë²¡í„° ìƒì„±
        async with httpx.AsyncClient(timeout=30.0) as client:
            clip_res = await client.post(
                f"{AI_SERVICE_API_URL}/generate-fashion-clip-vector",
                json={
                    "image_b64": request.image_b64,
                    "target": request.target
                }
            )
            
            if clip_res.status_code != 200:
                logger.warning("âš ï¸ Fashion CLIP endpoint failed, falling back to standard CLIP")
                clip_res = await client.post(
                    f"{AI_SERVICE_API_URL}/generate-clip-vector",
                    json={"image_b64": request.image_b64}
                )
            
            if clip_res.status_code != 200:
                raise HTTPException(status_code=500, detail="CLIP ë²¡í„° ìƒì„± ì‹¤íŒ¨")
            
            clip_data = clip_res.json()
            clip_vector = clip_data.get("vector", [])
            
            if not clip_vector or len(clip_vector) != 512:
                raise HTTPException(status_code=500, detail="ìœ íš¨í•˜ì§€ ì•Šì€ CLIP ë²¡í„°")
        
        logger.info(f"âœ… CLIP vector generated: {len(clip_vector)} dims (target: {request.target})")
        
        # 3. CLIP ë²¡í„°ë¡œ ìƒí’ˆ ê²€ìƒ‰
        results = await crud_product.search_by_clip_vector(
            db,
            clip_vector=clip_vector,
            limit=request.limit,
            filter_gender=target_gender,
            target=request.target,
            include_category=target_categories
        )
        
        logger.info(f"âœ… CLIP search found {len(results)} products (gender filter: {target_gender})")
        
        # 4. Response êµ¬ì„±
        product_responses = []
        for p in results:
            response = map_product_to_response(p)
            if response:
                product_responses.append(response)
        
        return {
            "status": "SUCCESS",
            "search_type": "CLIP_IMAGE_SIMILARITY",
            "products": product_responses
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ CLIP search failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/analyze-image")
async def analyze_image_proxy(request: ImageAnalysisRequest):
    """ê°œë³„ ì´ë¯¸ì§€ ë¶„ì„ í”„ë¡ì‹œ (í›„ë³´ ì´ë¯¸ì§€ ìƒì„¸ ë¶„ì„)"""
    
    # âœ… [FIX] URL ë³´ì • ë¡œì§ ì¶”ê°€
    raw_url = settings.AI_SERVICE_API_URL.rstrip("/")
    if "/api/v1" not in raw_url:
        AI_SERVICE_API_URL = f"{raw_url}/api/v1"
    else:
        AI_SERVICE_API_URL = raw_url

    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            target_url = f"{AI_SERVICE_API_URL}/analyze-image-detail"
            logger.info(f"ğŸ“¤ Calling AI Service: {target_url}")

            response = await client.post(
                target_url,
                json={"image_b64": request.image_b64, "query": request.query}
            )
            response.raise_for_status()
            return response.json()
    except httpx.HTTPStatusError as e:
        logger.error(f"âŒ AI Service HTTP Error: {e.response.status_code} - {e.response.text}")
        raise HTTPException(status_code=502, detail=f"AI Service Error: {e.response.status_code}")
    except Exception as e:
        logger.error(f"âŒ Analysis Proxy Failed: {e}")
        raise HTTPException(status_code=500, detail=f"AI Service Error: {str(e)}")


@router.post("/ai-search", response_model=Dict[str, Any])
async def ai_search(
    query: str = Form(..., description="ì‚¬ìš©ì ê²€ìƒ‰ ì¿¼ë¦¬"),
    image_file: Optional[UploadFile] = File(None),
    limit: int = Form(12),
    negative_prompt: Optional[str] = Form(None, description="ì œì™¸í•  í‚¤ì›Œë“œ (ì‰¼í‘œë¡œ êµ¬ë¶„)"),
    db: AsyncSession = Depends(deps.get_db),
) -> Any:
    """
    [Upgraded v3] ìŠ¤ë§ˆíŠ¸ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
    """
    logger.info(f"ğŸ” AI Search Request: '{query}' (Image: {image_file is not None}, Negative: {negative_prompt})")

    # 1. ì˜ë„ ë° ì •ë³´ ì¶”ì¶œ
    target_gender = detect_gender_intent(query)
    core_keyword = extract_core_keyword(query)
    is_celeb_search = is_celebrity_search(query)
    
    logger.info(f"ğŸ“Œ Gender: {target_gender}, Core Keyword: '{core_keyword}', Celebrity: {is_celeb_search}")

    # 2. ì—…ë¡œë“œ ì´ë¯¸ì§€ ì²˜ë¦¬
    image_b64: Optional[str] = None
    if image_file:
        try:
            content = await image_file.read()
            image_b64 = base64.b64encode(content).decode("utf-8")
        except Exception:
            raise HTTPException(status_code=400, detail="Invalid image file")

    # 3. AI Service í˜¸ì¶œ (ê²½ë¡œ íŒë‹¨ ë° ë²¡í„° ìƒì„±)
    # âœ… [FIX] URL ë³´ì • ë¡œì§ (í•µì‹¬ ìˆ˜ì • ì‚¬í•­)
    raw_url = settings.AI_SERVICE_API_URL.rstrip("/")
    if "/api/v1" not in raw_url:
        AI_SERVICE_API_URL = f"{raw_url}/api/v1"
    else:
        AI_SERVICE_API_URL = raw_url
    
    search_strategy = "SMART_HYBRID"
    search_path = "INTERNAL"
    ai_summary = "ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤."
    ref_image_url = None
    candidates = []
    
    bert_vec: Optional[List[float]] = None
    clip_vec: Optional[List[float]] = None
    
    max_retries = 3
    for attempt in range(max_retries):
        try:
            async with httpx.AsyncClient(timeout=120.0) as client:
                # 3-1. ê²½ë¡œ ê²°ì • API í˜¸ì¶œ
                # ì´ì œ AI_SERVICE_API_URL ë’¤ì— /api/v1ì´ ë¶™ì–´ìˆìœ¼ë¯€ë¡œ ì •ìƒ ì‘ë™í•¨
                path_res = await client.post(
                    f"{AI_SERVICE_API_URL}/determine-path",
                    json={"query": query}
                )
                
                search_path = "INTERNAL"
                if path_res.status_code == 200:
                    search_path = path_res.json().get("path", "INTERNAL")
                
                logger.info(f"ğŸ›¤ï¸ Search Path Decision: {search_path}")
                
                # 3-2. ê²½ë¡œì— ë”°ë¥¸ ìƒì„¸ ì²˜ë¦¬ í˜¸ì¶œ
                endpoint = "/process-external" if search_path == 'EXTERNAL' else "/process-internal"
                
                target_ai_url = f"{AI_SERVICE_API_URL}{endpoint}"
                
                payload = {"query": query, "image_b64": image_b64}
                ai_res = await client.post(target_ai_url, json=payload)
                ai_res.raise_for_status()
                
                data = ai_res.json()
                
                # ë²¡í„° ì¶”ì¶œ
                if "vectors" in data:
                    bert_vec = data["vectors"].get("bert")
                    clip_vec = data["vectors"].get("clip")
                    logger.info(f"ğŸ“Š Vectors received - BERT: {len(bert_vec) if bert_vec else 0}dim, CLIP: {len(clip_vec) if clip_vec else 0}dim")
                elif "vector" in data:
                    bert_vec = data["vector"]
                
                # AI ë¶„ì„ ê²°ê³¼ ì¶”ì¶œ
                if "ai_analysis" in data and data["ai_analysis"]:
                    analysis = data["ai_analysis"]
                    ai_summary = analysis.get("summary") or ai_summary
                    ref_image_url = analysis.get("reference_image")
                    candidates = analysis.get("candidates", [])
                else:
                    ai_summary = data.get("description") or data.get("reason") or ai_summary
                    ref_image_url = data.get("ref_image")
                
                search_strategy = data.get("strategy", search_path).upper()
                
                # ì™¸ë¶€ ì´ë¯¸ì§€ URLì´ë©´ í”„ë¡ì‹œ ì²˜ë¦¬
                if ref_image_url and ref_image_url.startswith("http"):
                    logger.info(f"ğŸ”„ Proxying reference image...")
                    proxy_image = await fetch_image_as_base64(ref_image_url)
                    if proxy_image:
                        ref_image_url = proxy_image
                
                break # ì„±ê³µ ì‹œ ì¬ì‹œë„ ë£¨í”„ íƒˆì¶œ

        except Exception as e:
            logger.warning(f"âš ï¸ AI Service Retry ({attempt+1}/{max_retries}): {e}")
            if attempt == max_retries - 1:
                search_strategy = "KEYWORD_FALLBACK"
                logger.error(f"âŒ AI Service failed after {max_retries} retries")
            await asyncio.sleep(1)

    # 4. ğŸŒŸ ê²€ìƒ‰ ì‹¤í–‰ - DB ì¡°íšŒ
    results = []
    gender_filtered = True # ì„±ë³„ í•„í„° ì ìš© ì—¬ë¶€ ì¶”ì 

    # ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸ê°€ ìˆìœ¼ë©´ ë” ë§ì€ í›„ë³´ ê²€ìƒ‰
    search_limit = limit * 2 if negative_prompt else limit

    try:
        # Case A: EXTERNAL ê²½ë¡œì´ë©´ì„œ CLIP ë²¡í„°ê°€ ìˆëŠ” ê²½ìš°
        if search_path == "EXTERNAL" and clip_vec and len(clip_vec) == 512:
            logger.info(f"ğŸ–¼ï¸ Using CLIP image vector search (512-dim)")

            results = await crud_product.search_by_clip_vector(
                db,
                clip_vector=clip_vec,
                limit=search_limit,
                filter_gender=target_gender
            )
            
            if results:
                search_strategy = "CLIP_VISUAL_SEARCH"
                logger.info(f"âœ… CLIP search found {len(results)} products")
            else:
                logger.info(f"âš ï¸ CLIP search empty, falling back to BERT")
                if bert_vec and len(bert_vec) == 768:
                    results = await crud_product.search_hybrid(
                        db,
                        bert_vector=bert_vec,
                        limit=search_limit,
                        filter_gender=target_gender
                    )
                    search_strategy = "BERT_FALLBACK"

        # Case B: INTERNAL ê²½ë¡œ ë˜ëŠ” ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ -> ìŠ¤ë§ˆíŠ¸ í•˜ì´ë¸Œë¦¬ë“œ
        if not results:
            results = await crud_product.search_smart_hybrid(
                db,
                query=core_keyword,
                bert_vector=bert_vec,
                clip_vector=clip_vec,
                limit=search_limit,
                filter_gender=target_gender
            )
            
            # ê²°ê³¼ ì—†ìœ¼ë©´ ì„±ë³„ í•„í„°ë¥¼ ì™„í™”
            if not results:
                logger.info(f"âš ï¸ No results with gender filter '{target_gender}', trying relaxed search")
                results = await crud_product.search_smart_hybrid(
                    db,
                    query=query,
                    bert_vector=bert_vec,
                    clip_vector=clip_vec,
                    limit=search_limit,
                    filter_gender=None
                )
                if results:
                    search_strategy = "RELAXED_SEARCH"
                    gender_filtered = False
                    logger.info(f"âš ï¸ Relaxed search found {len(results)} products (gender filter removed)")

        # Case C: ìµœí›„ì˜ ìˆ˜ë‹¨ (ìµœì‹  ìƒí’ˆ)
        if not results:
            results = await crud_product.get_multi(db, limit=search_limit)
            search_strategy = "FALLBACK_LATEST"
            gender_filtered = False

    except Exception as e:
        logger.error(f"âŒ DB Search Error: {e}")
        raise HTTPException(status_code=500, detail="Database Search Failed")

    # 5. ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸ í•„í„°ë§
    filtered_count = 0
    if negative_prompt and results:
        original_count = len(results)
        results = filter_by_negative_prompt(results, negative_prompt)
        filtered_count = original_count - len(results)
        logger.info(f"ğŸš« Negative filtering removed {filtered_count} products")

    # 6. ìµœì¢… ê²°ê³¼ ìë¥´ê¸°
    results = results[:limit]

    # 7. Response ë§¤í•‘
    product_responses = []
    for p in results:
        response = map_product_to_response(p)
        if response:
            product_responses.append(response)

    logger.info(f"âœ… Search Complete: {len(product_responses)} products found (Strategy: {search_strategy})")

    return {
        "status": "SUCCESS",
        "search_path": search_strategy,
        "gender_filter_applied": gender_filtered,
        "detected_gender": target_gender,
        "negative_prompt_applied": negative_prompt is not None and negative_prompt.strip() != "",
        "filtered_count": filtered_count,
        "ai_analysis": {
            "summary": ai_summary,
            "reference_image": ref_image_url,
            "candidates": candidates
        },
        "products": product_responses
    }