"""
search.py - ìˆ˜ì •ëœ ë²„ì „ v2
ê²½ë¡œ: backend-core/src/api/v1/endpoints/search.py

ìˆ˜ì • ì‚¬í•­:
1. í…ìŠ¤íŠ¸ë§Œ ìˆì–´ë„ determine-path í˜¸ì¶œ
2. EXTERNAL ê²½ë¡œì¼ ë•Œ CLIP ì´ë¯¸ì§€ ë²¡í„°ë¡œ ê²€ìƒ‰ (í•µì‹¬ ìˆ˜ì •!)
3. í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œ ì—°ì˜ˆì¸ ê²€ìƒ‰ ê³ ë ¤
"""

import logging
import base64
import asyncio
from typing import Optional, List, Dict, Any
from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, Form
from sqlalchemy.ext.asyncio import AsyncSession
import httpx
from pydantic import BaseModel, ValidationError 

from src.api import deps
from src.crud.crud_product import crud_product
from src.schemas.product import ProductResponse
from src.config.settings import settings
from src.models.product import Product  # âœ… Product ëª¨ë¸ import ì¶”ê°€

logger = logging.getLogger(__name__)
router = APIRouter()

# DTO
class ImageAnalysisRequest(BaseModel):
    image_b64: str
    query: str


def detect_gender_intent(query: str) -> Optional[str]:
    """ê²€ìƒ‰ì–´ì—ì„œ ì„±ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    q = query.lower()
    if any(x in q for x in ["ë‚¨ì", "ë‚¨ì„±", "ë§¨", "men", "male", "boy"]):
        return "Male"
    elif any(x in q for x in ["ì—¬ì", "ì—¬ì„±", "ìš°ë¨¼", "women", "female", "girl"]):
        return "Female"
    return None


def extract_core_keyword(query: str) -> str:
    """ê²€ìƒ‰ì–´ì—ì„œ í•µì‹¬ ìƒí’ˆ í‚¤ì›Œë“œ ì¶”ì¶œ (ì„±ë³„/ìˆ˜ì‹ì–´ ì œê±°)"""
    import re
    
    # ì œê±°í•  ë‹¨ì–´ë“¤
    remove_words = [
        "ë‚¨ì", "ì—¬ì", "ë‚¨ì„±", "ì—¬ì„±", "ë‚¨ì„±ìš©", "ì—¬ì„±ìš©",
        "ì¶”ì²œ", "í•´ì¤˜", "ë³´ì—¬ì¤˜", "ì°¾ì•„ì¤˜", "ì•Œë ¤ì¤˜",
        "ìŠ¤íƒ€ì¼", "íŒ¨ì…˜", "ì˜·", "ì˜ë¥˜", "ìš©"
    ]
    
    result = query
    for word in remove_words:
        result = result.replace(word, "")
    
    # ì¡°ì‚¬ ì œê±°
    result = re.sub(r'(ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì˜|ì—|ë¡œ)$', '', result.strip())
    
    return result.strip() if result.strip() else query


def is_celebrity_search(query: str) -> bool:
    """ì—°ì˜ˆì¸/ì¸ë¬¼ ê²€ìƒ‰ì¸ì§€ íŒë‹¨"""
    import re

    # íŒ¨ì…˜ ê´€ë ¨ í‚¤ì›Œë“œì™€ í•¨ê»˜ ì‚¬ìš©ëœ ê²½ìš°
    fashion_keywords = ["íŒ¨ì…˜", "ìŠ¤íƒ€ì¼", "ì½”ë””", "ë£©", "ê³µí•­", "ì°©ì¥", "ì˜ìƒ", "ì˜·"]

    # í•œê¸€ ì´ë¦„ íŒ¨í„´ (2-4ê¸€ì)
    korean_name = re.search(r'[ê°€-í£]{2,4}', query)

    if korean_name and any(k in query for k in fashion_keywords):
        return True

    return False


def extract_negative_keywords(negative_prompt: Optional[str]) -> List[str]:
    """
    ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸ì—ì„œ ì œì™¸í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
    ì˜ˆ: "ì²­ë°”ì§€, ìŠ¤ë‹ˆì»¤ì¦ˆ, ìºì£¼ì–¼" -> ["ì²­ë°”ì§€", "ìŠ¤ë‹ˆì»¤ì¦ˆ", "ìºì£¼ì–¼"]
    """
    if not negative_prompt:
        return []

    # ì‰¼í‘œ, ìŠ¬ë˜ì‹œ, ê³µë°±ìœ¼ë¡œ êµ¬ë¶„
    import re
    keywords = re.split(r'[,/\s]+', negative_prompt.strip())

    # ë¹ˆ ë¬¸ìì—´ ì œê±° ë° ì†Œë¬¸ì ë³€í™˜
    return [k.strip().lower() for k in keywords if k.strip()]


def filter_products_by_negative(products: List[Product], negative_keywords: List[str]) -> List[Product]:
    """
    ë„¤ê±°í‹°ë¸Œ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ìƒí’ˆ ì œì™¸
    """
    if not negative_keywords:
        return products

    filtered = []
    for product in products:
        # ìƒí’ˆëª…, ì„¤ëª…, ì¹´í…Œê³ ë¦¬ì—ì„œ ë„¤ê±°í‹°ë¸Œ í‚¤ì›Œë“œ ê²€ìƒ‰
        text_to_check = f"{product.name} {product.description or ''} {product.category or ''}".lower()

        # ë„¤ê±°í‹°ë¸Œ í‚¤ì›Œë“œê°€ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ë©´ ì œì™¸
        contains_negative = any(keyword in text_to_check for keyword in negative_keywords)

        if not contains_negative:
            filtered.append(product)

    logger.info(f"ğŸš« Filtered {len(products) - len(filtered)} products by negative keywords: {negative_keywords}")
    return filtered


async def fetch_image_as_base64(url: str) -> Optional[str]:
    """ì™¸ë¶€ ì´ë¯¸ì§€ í”„ë¡ì‹œ ë‹¤ìš´ë¡œë“œ"""
    if not url:
        return None
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Referer": "https://www.google.com/"
        }
        async with httpx.AsyncClient(timeout=5.0, verify=False) as client:
            response = await client.get(url, headers=headers)
            if response.status_code == 200:
                b64_data = base64.b64encode(response.content).decode('utf-8')
                content_type = response.headers.get("content-type", "image/jpeg")
                return f"data:{content_type};base64,{b64_data}"
    except Exception as e:
        logger.warning(f"âš ï¸ Failed to proxy image ({url}): {e}")
    return None


# âœ… NEW: CLIP ì´ë¯¸ì§€ ê¸°ë°˜ ê²€ìƒ‰ ì—”ë“œí¬ì¸íŠ¸
class ClipSearchRequest(BaseModel):
    image_b64: str
    limit: int = 12
    query: Optional[str] = None  # âœ… ì›ë³¸ ê²€ìƒ‰ì–´ (ì„±ë³„ ì¶”ì¶œìš©)
    target: str = "full"  # âœ… "full", "upper", "lower"
    negative_prompt: Optional[str] = None  # âœ… ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸ (ì œì™¸í•  íŠ¹ì§•)


@router.post("/search-by-clip")
async def search_by_clip_image(
    request: ClipSearchRequest,
    db: AsyncSession = Depends(deps.get_db),
):
    """
    ì´ë¯¸ì§€ ê¸°ë°˜ ìƒí’ˆ ê²€ìƒ‰
    - í›„ë³´ ì´ë¯¸ì§€ í´ë¦­ ì‹œ í˜¸ì¶œ
    - ì´ë¯¸ì§€ â†’ CLIP ë²¡í„° â†’ ìœ ì‚¬ ìƒí’ˆ ê²€ìƒ‰
    - âœ… ì›ë³¸ ì¿¼ë¦¬ì—ì„œ ì„±ë³„ ì¶”ì¶œí•˜ì—¬ í•„í„°ë§
    - âœ… target: "full"(ì „ì²´), "upper"(ìƒì˜), "lower"(í•˜ì˜)
    """
    logger.info(f"ğŸ–¼ï¸ CLIP Image Search Request (limit: {request.limit}, query: {request.query}, target: {request.target})")
    
    # âœ… ì›ë³¸ ì¿¼ë¦¬ì—ì„œ ì„±ë³„ ì¶”ì¶œ
    target_gender = None
    if request.query:
        target_gender = detect_gender_intent(request.query)
        logger.info(f"ğŸ“Œ Detected gender from query: {target_gender}")
    
    # âœ… targetì— ë”°ë¥¸ ì¹´í…Œê³ ë¦¬ í•„í„°
    category_filter = None
    if request.target == "upper":
        category_filter = ["Tops", "Outerwear", "Shirts", "Sweaters", "ìƒì˜", "ì•„ìš°í„°", "ì…”ì¸ ", "ë‹ˆíŠ¸"]
    elif request.target == "lower":
        category_filter = ["Bottoms", "Pants", "Skirts", "í•˜ì˜", "ë°”ì§€", "ì¹˜ë§ˆ"]
    
    AI_SERVICE_API_URL = settings.AI_SERVICE_API_URL
    
    try:
        # 1. AI ì„œë¹„ìŠ¤ì—ì„œ CLIP ë²¡í„° ìƒì„± (YOLO + ì˜ì—­ ì§€ì •)
        async with httpx.AsyncClient(timeout=30.0) as client:
            clip_res = await client.post(
                f"{AI_SERVICE_API_URL}/generate-fashion-clip-vector",
                json={
                    "image_b64": request.image_b64,
                    "target": request.target  # âœ… ì˜ì—­ ì§€ì •
                }
            )
            
            if clip_res.status_code != 200:
                # Fallback: ê¸°ì¡´ ì—”ë“œí¬ì¸íŠ¸
                logger.warning("âš ï¸ Fashion CLIP endpoint failed, falling back to standard CLIP")
                clip_res = await client.post(
                    f"{AI_SERVICE_API_URL}/generate-clip-vector",
                    json={"image_b64": request.image_b64}
                )
            
            if clip_res.status_code != 200:
                raise HTTPException(status_code=500, detail="CLIP ë²¡í„° ìƒì„± ì‹¤íŒ¨")
            
            clip_data = clip_res.json()
            clip_vector = clip_data.get("vector", [])
            
            if not clip_vector or len(clip_vector) != 512:
                raise HTTPException(status_code=500, detail="ìœ íš¨í•˜ì§€ ì•Šì€ CLIP ë²¡í„°")
        
        logger.info(f"âœ… CLIP vector generated: {len(clip_vector)} dims (target: {request.target})")
        
        # 2. CLIP ë²¡í„°ë¡œ ìƒí’ˆ ê²€ìƒ‰ (âœ… ì„±ë³„ í•„í„° ì ìš©)
        results = await crud_product.search_by_clip_vector(
            db,
            clip_vector=clip_vector,
            limit=request.limit * 2 if request.negative_prompt else request.limit,  # âœ… ë„¤ê±°í‹°ë¸Œ í•„í„°ë§ì„ ê³ ë ¤í•´ ë” ë§ì´ ê°€ì ¸ì˜´
            filter_gender=target_gender  # âœ… ì„±ë³„ í•„í„° ì¶”ê°€!
        )

        logger.info(f"âœ… CLIP search found {len(results)} products (gender filter: {target_gender})")

        # âœ… ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸ í•„í„°ë§
        if request.negative_prompt:
            negative_keywords = extract_negative_keywords(request.negative_prompt)
            results = filter_products_by_negative(results, negative_keywords)
            results = results[:request.limit]  # í•„í„°ë§ í›„ ì›ë˜ limitìœ¼ë¡œ ì œí•œ

        # 3. Response êµ¬ì„±
        product_responses = []
        for p in results:
            try:
                p_dict = {
                    "id": p.id,
                    "name": p.name or "Unnamed Product",
                    "description": p.description or "",
                    "price": float(p.price) if p.price else 0,
                    "stock_quantity": int(p.stock_quantity) if p.stock_quantity else 0,
                    "category": p.category or "Etc",
                    "image_url": p.image_url or "",
                    "gender": p.gender or "Unisex",
                    "is_active": p.is_active if p.is_active is not None else True,
                    "created_at": p.created_at,
                    "updated_at": p.updated_at,
                    "in_stock": (p.stock_quantity or 0) > 0
                }
                validated_product = ProductResponse.model_validate(p_dict)
                product_responses.append(validated_product)
            except ValidationError:
                continue
        
        return {
            "status": "SUCCESS",
            "search_type": "CLIP_IMAGE_SIMILARITY",
            "products": product_responses
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ CLIP search failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/analyze-image")
async def analyze_image_proxy(request: ImageAnalysisRequest):
    """ê°œë³„ ì´ë¯¸ì§€ ë¶„ì„ í”„ë¡ì‹œ (í›„ë³´ ì´ë¯¸ì§€ ìƒì„¸ ë¶„ì„)"""
    AI_SERVICE_API_URL = settings.AI_SERVICE_API_URL.rstrip("/")
    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            # âœ… /analyze-image-detail ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ (JSON body ë²„ì „)
            target_url = f"{AI_SERVICE_API_URL}/analyze-image-detail"
            if "/api/v1" not in AI_SERVICE_API_URL:
                target_url = f"{AI_SERVICE_API_URL}/api/v1/analyze-image-detail"
            
            logger.info(f"ğŸ“¤ Calling AI Service: {target_url}")

            response = await client.post(
                target_url,
                json={"image_b64": request.image_b64, "query": request.query}
            )
            response.raise_for_status()
            return response.json()
    except httpx.HTTPStatusError as e:
        logger.error(f"âŒ AI Service HTTP Error: {e.response.status_code} - {e.response.text}")
        raise HTTPException(status_code=502, detail=f"AI Service Error: {e.response.status_code}")
    except Exception as e:
        logger.error(f"âŒ Analysis Proxy Failed: {e}")
        raise HTTPException(status_code=500, detail=f"AI Service Error: {str(e)}")


@router.post("/ai-search", response_model=Dict[str, Any])
async def ai_search(
    query: str = Form(..., description="ì‚¬ìš©ì ê²€ìƒ‰ ì¿¼ë¦¬"),
    image_file: Optional[UploadFile] = File(None),
    limit: int = Form(12),
    negative_prompt: Optional[str] = Form(None, description="ì œì™¸í•  íŠ¹ì§•/ìŠ¤íƒ€ì¼"),
    db: AsyncSession = Depends(deps.get_db),
) -> Any:
    """
    [Upgraded v2] ìŠ¤ë§ˆíŠ¸ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
    - í‚¤ì›Œë“œ ë§¤ì¹­ ìš°ì„  (ì¼ë°˜ ê²€ìƒ‰)
    - âœ… EXTERNAL ê²½ë¡œ: CLIP ì´ë¯¸ì§€ ë²¡í„°ë¡œ ì‹œê°ì  ìœ ì‚¬ë„ ê²€ìƒ‰
    - ì„±ë³„ í•„í„° ìë™ ì ìš©
    """
    logger.info(f"ğŸ” AI Search Request: '{query}' (Image: {image_file is not None})")

    # 1. ì˜ë„ íŒŒì•…
    target_gender = detect_gender_intent(query)
    core_keyword = extract_core_keyword(query)
    is_celeb_search = is_celebrity_search(query)
    
    logger.info(f"ğŸ“Œ Gender: {target_gender}, Core Keyword: '{core_keyword}', Celebrity: {is_celeb_search}")

    # 2. ì´ë¯¸ì§€ ì²˜ë¦¬
    image_b64: Optional[str] = None
    if image_file:
        try:
            content = await image_file.read()
            image_b64 = base64.b64encode(content).decode("utf-8")
        except Exception:
            raise HTTPException(status_code=400, detail="Invalid image file")

    # 3. AI Service í˜¸ì¶œ
    AI_SERVICE_API_URL = settings.AI_SERVICE_API_URL
    
    search_strategy = "SMART_HYBRID"
    search_path = "INTERNAL"
    ai_summary = "ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤."
    ref_image_url = None
    candidates = []
    
    bert_vec: Optional[List[float]] = None
    clip_vec: Optional[List[float]] = None
    
    # âœ… í•­ìƒ determine-path í˜¸ì¶œ
    max_retries = 3
    for attempt in range(max_retries):
        try:
            async with httpx.AsyncClient(timeout=120.0) as client:
                # ê²½ë¡œ ê²°ì • API í˜¸ì¶œ
                path_res = await client.post(
                    f"{AI_SERVICE_API_URL}/determine-path",
                    json={"query": query}
                )
                search_path = "INTERNAL"
                if path_res.status_code == 200:
                    search_path = path_res.json().get("path", "INTERNAL")
                
                logger.info(f"ğŸ›¤ï¸ Search Path Decision: {search_path}")
                
                # ê²½ë¡œì— ë”°ë¼ ì ì ˆí•œ ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ
                endpoint = "/process-external" if search_path == 'EXTERNAL' else "/process-internal"
                payload = {"query": query, "image_b64": image_b64}
                
                ai_res = await client.post(f"{AI_SERVICE_API_URL}{endpoint}", json=payload)
                ai_res.raise_for_status()
                
                data = ai_res.json()
                
                # ë²¡í„° ì¶”ì¶œ
                if "vectors" in data:
                    bert_vec = data["vectors"].get("bert")
                    clip_vec = data["vectors"].get("clip")
                    logger.info(f"ğŸ“Š Vectors received - BERT: {len(bert_vec) if bert_vec else 0}dim, CLIP: {len(clip_vec) if clip_vec else 0}dim")
                elif "vector" in data:
                    bert_vec = data["vector"]
                
                # AI ë¶„ì„ ê²°ê³¼ ì¶”ì¶œ
                if "ai_analysis" in data and data["ai_analysis"]:
                    analysis = data["ai_analysis"]
                    ai_summary = analysis.get("summary") or ai_summary
                    ref_image_url = analysis.get("reference_image")
                    candidates = analysis.get("candidates", [])
                else:
                    ai_summary = data.get("description") or data.get("reason") or ai_summary
                    ref_image_url = data.get("ref_image")
                
                search_strategy = data.get("strategy", search_path).upper()
                
                # ì™¸ë¶€ ì´ë¯¸ì§€ URLì´ë©´ í”„ë¡ì‹œ ì²˜ë¦¬
                if ref_image_url and ref_image_url.startswith("http"):
                    logger.info(f"ğŸ”„ Proxying reference image...")
                    proxy_image = await fetch_image_as_base64(ref_image_url)
                    if proxy_image:
                        ref_image_url = proxy_image
                
                break  # ì„±ê³µ ì‹œ ë£¨í”„ íƒˆì¶œ

        except Exception as e:
            logger.warning(f"âš ï¸ AI Service Retry ({attempt+1}/{max_retries}): {e}")
            if attempt == max_retries - 1:
                search_strategy = "KEYWORD_FALLBACK"
                logger.error(f"âŒ AI Service failed after {max_retries} retries")
            await asyncio.sleep(1)

    # 4. ğŸŒŸ ê²€ìƒ‰ ì‹¤í–‰ - ê²½ë¡œì— ë”°ë¼ ë‹¤ë¥¸ ì „ëµ
    results = []
    
    try:
        # âœ… í•µì‹¬ ìˆ˜ì •: EXTERNAL ê²½ë¡œ (ì—°ì˜ˆì¸ íŒ¨ì…˜ ë“±) â†’ CLIP ì´ë¯¸ì§€ ë²¡í„°ë¡œ ê²€ìƒ‰
        if search_path == "EXTERNAL" and clip_vec and len(clip_vec) == 512:
            logger.info(f"ğŸ–¼ï¸ Using CLIP image vector search (512-dim)")
            
            # CLIP ì´ë¯¸ì§€ ë²¡í„°ë¡œ ì‹œê°ì  ìœ ì‚¬ë„ ê²€ìƒ‰
            results = await crud_product.search_by_clip_vector(
                db,
                clip_vector=clip_vec,
                limit=limit,
                filter_gender=target_gender
            )
            
            if results:
                search_strategy = "CLIP_VISUAL_SEARCH"
                logger.info(f"âœ… CLIP search found {len(results)} products")
            else:
                # CLIP ê²€ìƒ‰ ê²°ê³¼ ì—†ìœ¼ë©´ BERTë¡œ Fallback
                logger.info(f"âš ï¸ CLIP search empty, falling back to BERT")
                if bert_vec and len(bert_vec) == 768:
                    results = await crud_product.search_hybrid(
                        db,
                        bert_vector=bert_vec,
                        limit=limit,
                        filter_gender=target_gender
                    )
                    search_strategy = "BERT_FALLBACK"
        
        # INTERNAL ê²½ë¡œ ë˜ëŠ” CLIP ë²¡í„° ì—†ìŒ â†’ ê¸°ì¡´ ìŠ¤ë§ˆíŠ¸ í•˜ì´ë¸Œë¦¬ë“œ
        if not results:
            results = await crud_product.search_smart_hybrid(
                db,
                query=core_keyword,
                bert_vector=bert_vec,
                clip_vector=clip_vec,
                limit=limit,
                filter_gender=target_gender
            )
            
            # ê²°ê³¼ ì—†ìœ¼ë©´ ì „ì²´ ì¿¼ë¦¬ë¡œ ì¬ì‹œë„
            if not results:
                results = await crud_product.search_smart_hybrid(
                    db,
                    query=query,
                    bert_vector=bert_vec,
                    clip_vector=clip_vec,
                    limit=limit,
                    filter_gender=None
                )
                if results:
                    search_strategy = "RELAXED_SEARCH"
        
        # ê·¸ë˜ë„ ì—†ìœ¼ë©´ ìµœì‹  ìƒí’ˆ
        if not results:
            results = await crud_product.get_multi(db, limit=limit)
            search_strategy = "FALLBACK_LATEST"

    except Exception as e:
        logger.error(f"âŒ DB Search Error: {e}")
        raise HTTPException(status_code=500, detail="Database Search Failed")

    # âœ… ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸ í•„í„°ë§
    if negative_prompt:
        negative_keywords = extract_negative_keywords(negative_prompt)
        logger.info(f"ğŸš« Applying negative filter: {negative_keywords}")
        results = filter_products_by_negative(results, negative_keywords)

    # 5. Response êµ¬ì„±
    product_responses = []
    for p in results:
        try:
            p_dict = {
                "id": p.id,
                "name": p.name or "Unnamed Product",
                "description": p.description or "",
                "price": float(p.price) if p.price else 0,
                "stock_quantity": int(p.stock_quantity) if p.stock_quantity else 0,
                "category": p.category or "Etc",
                "image_url": p.image_url or "",
                "gender": p.gender or "Unisex",
                "is_active": p.is_active if p.is_active is not None else True,
                "created_at": p.created_at,
                "updated_at": p.updated_at,
                "in_stock": (p.stock_quantity or 0) > 0
            }
            validated_product = ProductResponse.model_validate(p_dict)
            product_responses.append(validated_product)
        except ValidationError:
            continue

    logger.info(f"âœ… Search Complete: {len(product_responses)} products found (Strategy: {search_strategy})")

    return {
        "status": "SUCCESS",
        "search_path": search_strategy,
        "ai_analysis": {
            "summary": ai_summary,
            "reference_image": ref_image_url,
            "candidates": candidates
        },
        "products": product_responses
    }


# âœ… [NEW] ë¬´ë“œ ê¸°ë°˜ ê²€ìƒ‰ (í–¥ìˆ˜/ìŒì•… â†’ íŒ¨ì…˜)
@router.post("/mood-search", response_model=Dict[str, Any])
async def mood_search(
    query: str = Form(..., description="í–¥ìˆ˜ ë…¸íŠ¸ ë˜ëŠ” ìŒì•… ì¥ë¥´"),
    limit: int = Form(12),
    db: AsyncSession = Depends(deps.get_db),
) -> Any:
    """
    ë¬´ë“œ ê¸°ë°˜ íŒ¨ì…˜ ê²€ìƒ‰
    - í–¥ìˆ˜ ë…¸íŠ¸: "í”Œë¡œëŸ´", "ìš°ë””", "ì‹œíŠ¸ëŸ¬ìŠ¤" ë“±
    - ìŒì•… ì¥ë¥´: "ì¬ì¦ˆ", "í™í•©", "í´ë˜ì‹" ë“±
    - ìë™ìœ¼ë¡œ ì–´ìš¸ë¦¬ëŠ” íŒ¨ì…˜ ìŠ¤íƒ€ì¼ ì¶”ì²œ
    """
    from src.utils.mood_mapper import extract_mood_keywords, build_mood_search_query

    logger.info(f"ğŸ­ Mood Search Request: '{query}'")

    # 1. ë¬´ë“œ í‚¤ì›Œë“œ ì¶”ì¶œ
    mood_data = extract_mood_keywords(query)

    if not mood_data.get("detected"):
        # ê°ì§€ëœ ë¬´ë“œê°€ ì—†ìœ¼ë©´ ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ fallback
        logger.info(f"âš ï¸ No mood detected, falling back to regular search")
        return {
            "status": "NO_MOOD_DETECTED",
            "message": f"'{query}'ì—ì„œ í–¥ìˆ˜ ë…¸íŠ¸ë‚˜ ìŒì•… ì¥ë¥´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ ê²€ìƒ‰ì„ ì‹œë„í•˜ì„¸ìš”.",
            "products": []
        }

    logger.info(f"âœ… Mood detected: {mood_data['detected']} ({mood_data['type']})")

    # 2. ë¬´ë“œ ë°ì´í„°ë¡œë¶€í„° ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
    search_query = build_mood_search_query(mood_data)
    negative_keywords_str = ", ".join(mood_data.get("negative", []))

    logger.info(f"ğŸ” Generated query: '{search_query}' (negative: '{negative_keywords_str}')")

    # 3. ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰
    try:
        # í‚¤ì›Œë“œ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        results = await crud_product.search_smart_hybrid(
            db,
            query=search_query,
            limit=limit * 2 if negative_keywords_str else limit  # ë„¤ê±°í‹°ë¸Œ í•„í„° ê³ ë ¤
        )

        logger.info(f"âœ… Found {len(results)} products")

        # 4. ë„¤ê±°í‹°ë¸Œ í•„í„°ë§
        if negative_keywords_str:
            negative_keywords = extract_negative_keywords(negative_keywords_str)
            results = filter_products_by_negative(results, negative_keywords)
            results = results[:limit]

        # 5. Response êµ¬ì„±
        product_responses = []
        for p in results:
            try:
                p_dict = {
                    "id": p.id,
                    "name": p.name or "Unnamed Product",
                    "description": p.description or "",
                    "price": float(p.price) if p.price else 0,
                    "stock_quantity": int(p.stock_quantity) if p.stock_quantity else 0,
                    "category": p.category or "Etc",
                    "image_url": p.image_url or "",
                    "gender": p.gender or "Unisex",
                    "is_active": p.is_active if p.is_active is not None else True,
                    "created_at": p.created_at,
                    "updated_at": p.updated_at,
                    "in_stock": (p.stock_quantity or 0) > 0
                }
                validated_product = ProductResponse.model_validate(p_dict)
                product_responses.append(validated_product)
            except ValidationError:
                continue

        # 6. ë¬´ë“œ ì„¤ëª… ìƒì„±
        mood_description = f"{mood_data['detected']}ì˜ {mood_data['mood']} ë¶„ìœ„ê¸°ì— ì–´ìš¸ë¦¬ëŠ” {mood_data['style']} ìŠ¤íƒ€ì¼ì…ë‹ˆë‹¤."

        return {
            "status": "SUCCESS",
            "mood_info": {
                "type": mood_data["type"],  # "perfume" or "music"
                "detected": mood_data["detected"],
                "style": mood_data["style"],
                "mood": mood_data["mood"],
                "colors": mood_data["colors"],
                "description": mood_description
            },
            "products": product_responses
        }

    except Exception as e:
        logger.error(f"âŒ Mood search failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))